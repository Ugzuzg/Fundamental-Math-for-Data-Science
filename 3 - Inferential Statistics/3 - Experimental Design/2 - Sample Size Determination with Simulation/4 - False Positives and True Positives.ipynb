{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# False Positives and True Positives\n",
    "\n",
    "In the previous exercise, we simulated 1,000 datasets and ran a Chi-Square test for each one, recording whether the results were ‘significant’ or ‘not significant’. This allowed us to estimate the proportion of simulated datasets that led to a ‘significant’ result.\n",
    "\n",
    "In general, we hope that the test reflects reality. We therefore want the result to be ‘significant’ if there really is a significant difference in the probability of an open for the two email subjects (lift > 0). In that case, the proportion of significant results is the true positive rate, also called the power of the test. Most sample size calculators aim for a power of 80%.\n",
    "\n",
    "On the other hand, if there’s no difference in the probability of an email being opened for the two email subjects (lift = 0), a ‘significant’ result would be a false-positive (also called a type I error). This would lead us to invest time and resources into adding first names into email subjects when there’s no real pay-off in the long run.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "\n",
    "1. The simulation code from the previous exercises is loaded for you in script.py. We’ve included the code to print out the proportion of tests where a significant result was recorded. Currently, the simulation is set up so that there is a difference in the probability of a subscription for the two buttons.\n",
    "\n",
    "    Press “Run” a few times and inspect the proportion of significant tests (printed to the output terminal) each time. If we ran a test with the provided sample size (100), baseline conversion rate (50%) and lift (30%), approximately what percent of the time would we correctly observe a significant result? Note that this is the “power” of the test.\n",
    "\n",
    "    <details>\n",
    "        <summary>Stuck? Get a hint</summary>\n",
    "    \n",
    "    When we run this code, we see that approximately 28% of the tests result in a significant p-value. Every time we run the simulation, we’ll get slightly different numbers, but they’re all roughly between .20 and .36.\n",
    "    </details>\n",
    "\n",
    "2. Now, change the value of `lift` so that the proportion of significant tests is equal to the **false positive rate** and press “Run” once more.\n",
    "\n",
    "    Note that the proportion of significant tests should be approximately equal to the significance threshold if you’ve done this correctly.\n",
    "\n",
    "    <details>\n",
    "        <summary>Stuck? Get a hint</summary>\n",
    "    \n",
    "    If the proportion of significant tests is equal to the **false positive rate**, that means the significant results are wrong and there is no difference between the groups. This means `lift` should be `0`.\n",
    "\n",
    "    Remember that lift is the inherent difference between the groups. Here, a lift of 0% will mean that we are sampling from populations that have an equal probability of a “success”.\n",
    "    </details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "# preset values\n",
    "significance_threshold = 0.05\n",
    "sample_size = 100\n",
    "lift = .3\n",
    "control_rate = .5\n",
    "name_rate = (1 + lift) * control_rate\n",
    "\n",
    "# initialize an empty list of results\n",
    "results = []\n",
    "\n",
    "# start the loop\n",
    "for i in range(100):\n",
    "  # simulate data:\n",
    "  sample_control = np.random.choice(['yes', 'no'],  size=int(sample_size/2), p=[control_rate, 1-control_rate])\n",
    "  sample_name = np.random.choice(['yes', 'no'], size=int(sample_size/2), p=[name_rate, 1-name_rate])\n",
    "  group = ['control']*int(sample_size/2) + ['name']*int(sample_size/2)\n",
    "  outcome = list(sample_control) + list(sample_name)\n",
    "  sim_data = {\"Email\": group, \"Opened\": outcome}\n",
    "  sim_data = pd.DataFrame(sim_data)\n",
    "\n",
    "  # run the test\n",
    "  ab_contingency = pd.crosstab(np.array(sim_data.Email), np.array(sim_data.Opened))\n",
    "  chi2, pval, dof, expected = chi2_contingency(ab_contingency)\n",
    "  result = ('significant' if pval < significance_threshold else 'not significant')\n",
    "\n",
    "  # append the result to our results list:\n",
    "  results.append(result)\n",
    "\n",
    "# calculate proportion of significant results:\n",
    "print(\"Proportion of significant results:\")\n",
    "results =  np.array(results)\n",
    "print(np.sum(results == 'significant')/100)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
  },
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
